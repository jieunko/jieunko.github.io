<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://jieunko.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://jieunko.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-06T11:50:00+00:00</updated><id>https://jieunko.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">NeRF_Representing Scenes as Neural Radiance Fields for View Synthesis</title><link href="https://jieunko.github.io/blog/2025/plotly/" rel="alternate" type="text/html" title="NeRF_Representing Scenes as Neural Radiance Fields for View Synthesis"/><published>2025-06-05T20:13:00+00:00</published><updated>2025-06-05T20:13:00+00:00</updated><id>https://jieunko.github.io/blog/2025/plotly</id><content type="html" xml:base="https://jieunko.github.io/blog/2025/plotly/"><![CDATA[<h3 id="definition-and-core-concept-of-nerf">Definition and Core Concept of NeRF</h3> <p>A Neural Radiance Field (NeRF) is a state-of-the-art artificial intelligence technique developed to create photorealistic 3D scene representations from a set of regular 2D photographs taken from different angles, often accompanied by camera position data (What Is NeRF? - Neural Radiance Fields Explained - AWS, 2022). Unlike traditional 3D modeling approaches that rely on manually constructed meshes or point clouds, NeRF utilizes deep neural networks—typically a multilayer perceptron (MLP)—to learn an implicit, continuous volumetric function that encodes both the shape (geometry) and the appearance (color, reflectivity, etc.) of the scene (NeRF Explained - Neural Radiance Field - Papers With Code, 2018).</p> <h3 id="how-nerf-represents-scenes">How NeRF Represents Scenes</h3> <p>At its core, NeRF represents a scene with a continuous function that takes as input a specific 3D point’s spatial coordinates (x, y, z) and a viewing direction (usually described by two angles, forming a 5D input vector) and outputs the corresponding color (RGB) and volume density (opacity) of that point (NeRF Explained - Neural Radiance Field - Papers With Code, 2018). This means, for any given point in 3D space, NeRF can predict how much light is emitted or reflected towards the viewer, effectively simulating how real eyes or a camera would perceive the scene from various viewpoints (Deep Dive into NeRF (Neural Radiance Fields), 2022).</p> <h3 id="training-process">Training Process</h3> <p>NeRF is trained using a collection of 2D images of the object or scene alongside the camera pose information for each image (Deep Gan Team, 2023). For each pixel in every image, rays are mathematically cast from the camera into the 3D scene, and points sampled along these rays are input into the neural network (Deep Dive into NeRF (Neural Radiance Fields), 2022). The MLP predicts the color and density for each sampled point, and this data is composited using volume rendering techniques to simulate how the light would travel along the ray and produce the observed pixel value in the image (What Is NeRF? - Neural Radiance Fields Explained - AWS, 2022). The network is optimized by minimizing the difference between the predicted pixel values and the ground truth pixels, allowing it to encode an accurate internal model of the scene (Deep Dive into NeRF (Neural Radiance Fields), 2022).</p> <h3 id="rendering-and-view-synthesis">Rendering and View Synthesis</h3> <p>Once trained, NeRF can render images of the captured scene from any viewpoint, not just those used during training (Deep Gan Team, 2023). It achieves this by querying the neural network for new sets of 3D coordinates and viewing angles, producing the appropriate RGB and density values, and performing volume rendering to synthesize the new views (What Is NeRF? - Neural Radiance Fields Explained - AWS, 2022). This novel view synthesis capability allows immersive exploration and visualization of digital content, opening up a wide range of applications (What Is NeRF? - Neural Radiance Fields Explained - AWS, 2022).</p> <h3 id="unique-technical-innovations">Unique Technical Innovations</h3> <p>One of NeRF’s hallmark innovations is its use of positional encoding, which maps the input spatial coordinates and viewing directions to a higher-dimensional space using functions such as sine and cosine (Deep Dive into NeRF (Neural Radiance Fields), 2022). This encoding helps the network to learn complex scene details and capture high-frequency information like fine textures and subtle changes in lighting (Deep Dive into NeRF (Neural Radiance Fields), 2022).</p> <p>In addition, NeRF employs hierarchical volume sampling strategies, such as coarse-to-fine sampling along rays, to increase rendering efficiency without sacrificing image quality (Deep Dive into NeRF (Neural Radiance Fields), 2022). This involves allocating more computational resources to spatial regions of interest, such as surfaces, and skipping empty space when feasible, thus optimizing inference and rendering times (What Is NeRF? - Neural Radiance Fields Explained - AWS, 2022).</p> <h3 id="advantages-over-traditional-3d-modeling">Advantages Over Traditional 3D Modeling</h3> <p>NeRF offers several advantages over conventional methods like photogrammetry or mesh modeling:</p> <p>It can naturally model complex view-dependent effects like reflections, refractions, and translucency, which are generally problematic for traditional mesh- or voxel-based approaches (Neural Radiance Fields (NeRF) Explained - Ultralytics, 2024).</p> <p>The implicit neural network-based representation is memory-efficient and can capture arbitrarily complex scenes without having to manually specify geometry or lighting (Neural Radiance Fields (NeRF) Explained - Ultralytics, 2024).</p> <p>NeRF can synthesize highly photorealistic new viewpoints and even fill in missing data between original images, a task difficult for methods reliant on dense correspondence or explicit surface reconstruction (NeRF Explained - Neural Radiance Field - Papers With Code, 2018).</p> <h3 id="limitations-of-nerf">Limitations of NeRF</h3> <p>Despite its strengths, NeRF is not without limitations:</p> <p>The computational demands for training and rendering are high—rendering a single novel viewpoint traditionally involved making multiple forward passes through the neural network for each pixel, making real-time applications challenging without further optimizations (Deep Gan Team, 2023).</p> <p>NeRF’s original formulation assumes the scene is completely static and does not handle dynamic or articulated scenes well, though more recent research is addressing these issues (arabbyuab.edu, 2024).</p> <p>The technique requires accurate camera poses for each input image, and the results are sensitive to the quantity and quality of training data (Deep Dive into NeRF (Neural Radiance Fields), 2022).</p> <h3 id="applications-of-nerf">Applications of NeRF</h3> <p>NeRF’s ability to generate photorealistic 3D visualizations from simple 2D data has spurred innovation across many domains:</p> <p>Visual effects in film and television, where NeRF can generate rich virtual sets, insert synthetic actors, or create digital doubles quickly (Neural Radiance Fields (NeRF) Explained - Ultralytics, 2024).</p> <p>Virtual and augmented reality, enabling users to explore captured environments from any viewpoint, significantly enhancing immersion (What Is NeRF? - Neural Radiance Fields Explained - AWS, 2022).</p> <p>Robotics and simulation, where realistic 3D scene modeling improves perception and navigation capabilities (What Is NeRF? - Neural Radiance Fields Explained - AWS, 2022).</p> <p>Cultural heritage preservation, digital archiving, and virtual tourism, making detailed 3D records accessible to broader audiences (The NeRF Revolution: Neural Radiance Fields Explained, 2024).</p> <h3 id="conclusion">Conclusion</h3> <p>NeRF is a groundbreaking machine learning technique that bridges 2D image datasets and fully immersive 3D environments, utilizing neural networks and advanced volume rendering to achieve levels of photorealism and flexibility previously unattainable by traditional methods (What Is NeRF? - Neural Radiance Fields Explained - AWS, 2022). While computationally intensive and best suited for static scenes in its original form, NeRF continues to evolve, shaping the future of digital content creation and 3D visualization (The NeRF Revolution: Neural Radiance Fields Explained, 2024).</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="paper-summary"/><category term="nerf"/><category term="AI-generated"/><summary type="html"><![CDATA[Brief explanation of Nerf [Liner generated]]]></summary></entry></feed>