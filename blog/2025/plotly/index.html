<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> NeRF_Representing Scenes as Neural Radiance Fields for View Synthesis | Jieun Ko </title> <meta name="author" content="Jieun Ko"> <meta name="description" content="Brief explanation of Nerf [Liner generated]"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jieunko.github.io/blog/2025/plotly/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Jieun</span> Ko </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">NeRF_Representing Scenes as Neural Radiance Fields for View Synthesis</h1> <p class="post-meta"> Created on June 05, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/paper-summary"> <i class="fa-solid fa-hashtag fa-sm"></i> paper-summary</a>   <a href="/blog/tag/nerf"> <i class="fa-solid fa-hashtag fa-sm"></i> nerf</a>   <a href="/blog/tag/ai-generated"> <i class="fa-solid fa-hashtag fa-sm"></i> AI-generated</a>   ·   <a href="/blog/category/sample-posts"> <i class="fa-solid fa-tag fa-sm"></i> sample-posts</a> </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h3"><a href="#definition-and-core-concept-of-nerf">Definition and Core Concept of NeRF</a></li> <li class="toc-entry toc-h3"><a href="#how-nerf-represents-scenes">How NeRF Represents Scenes</a></li> <li class="toc-entry toc-h3"><a href="#training-process">Training Process</a></li> <li class="toc-entry toc-h3"><a href="#rendering-and-view-synthesis">Rendering and View Synthesis</a></li> <li class="toc-entry toc-h3"><a href="#unique-technical-innovations">Unique Technical Innovations</a></li> <li class="toc-entry toc-h3"><a href="#advantages-over-traditional-3d-modeling">Advantages Over Traditional 3D Modeling</a></li> <li class="toc-entry toc-h3"><a href="#limitations-of-nerf">Limitations of NeRF</a></li> <li class="toc-entry toc-h3"><a href="#applications-of-nerf">Applications of NeRF</a></li> <li class="toc-entry toc-h3"><a href="#conclusion">Conclusion</a></li> </ul> </div> <hr> <div id="markdown-content"> <h3 id="definition-and-core-concept-of-nerf">Definition and Core Concept of NeRF</h3> <p>A Neural Radiance Field (NeRF) is a state-of-the-art artificial intelligence technique developed to create photorealistic 3D scene representations from a set of regular 2D photographs taken from different angles, often accompanied by camera position data (What Is NeRF? - Neural Radiance Fields Explained - AWS, 2022). Unlike traditional 3D modeling approaches that rely on manually constructed meshes or point clouds, NeRF utilizes deep neural networks—typically a multilayer perceptron (MLP)—to learn an implicit, continuous volumetric function that encodes both the shape (geometry) and the appearance (color, reflectivity, etc.) of the scene (NeRF Explained - Neural Radiance Field - Papers With Code, 2018).</p> <h3 id="how-nerf-represents-scenes">How NeRF Represents Scenes</h3> <p>At its core, NeRF represents a scene with a continuous function that takes as input a specific 3D point’s spatial coordinates (x, y, z) and a viewing direction (usually described by two angles, forming a 5D input vector) and outputs the corresponding color (RGB) and volume density (opacity) of that point (NeRF Explained - Neural Radiance Field - Papers With Code, 2018). This means, for any given point in 3D space, NeRF can predict how much light is emitted or reflected towards the viewer, effectively simulating how real eyes or a camera would perceive the scene from various viewpoints (Deep Dive into NeRF (Neural Radiance Fields), 2022).</p> <h3 id="training-process">Training Process</h3> <p>NeRF is trained using a collection of 2D images of the object or scene alongside the camera pose information for each image (Deep Gan Team, 2023). For each pixel in every image, rays are mathematically cast from the camera into the 3D scene, and points sampled along these rays are input into the neural network (Deep Dive into NeRF (Neural Radiance Fields), 2022). The MLP predicts the color and density for each sampled point, and this data is composited using volume rendering techniques to simulate how the light would travel along the ray and produce the observed pixel value in the image (What Is NeRF? - Neural Radiance Fields Explained - AWS, 2022). The network is optimized by minimizing the difference between the predicted pixel values and the ground truth pixels, allowing it to encode an accurate internal model of the scene (Deep Dive into NeRF (Neural Radiance Fields), 2022).</p> <h3 id="rendering-and-view-synthesis">Rendering and View Synthesis</h3> <p>Once trained, NeRF can render images of the captured scene from any viewpoint, not just those used during training (Deep Gan Team, 2023). It achieves this by querying the neural network for new sets of 3D coordinates and viewing angles, producing the appropriate RGB and density values, and performing volume rendering to synthesize the new views (What Is NeRF? - Neural Radiance Fields Explained - AWS, 2022). This novel view synthesis capability allows immersive exploration and visualization of digital content, opening up a wide range of applications (What Is NeRF? - Neural Radiance Fields Explained - AWS, 2022).</p> <h3 id="unique-technical-innovations">Unique Technical Innovations</h3> <p>One of NeRF’s hallmark innovations is its use of positional encoding, which maps the input spatial coordinates and viewing directions to a higher-dimensional space using functions such as sine and cosine (Deep Dive into NeRF (Neural Radiance Fields), 2022). This encoding helps the network to learn complex scene details and capture high-frequency information like fine textures and subtle changes in lighting (Deep Dive into NeRF (Neural Radiance Fields), 2022).</p> <p>In addition, NeRF employs hierarchical volume sampling strategies, such as coarse-to-fine sampling along rays, to increase rendering efficiency without sacrificing image quality (Deep Dive into NeRF (Neural Radiance Fields), 2022). This involves allocating more computational resources to spatial regions of interest, such as surfaces, and skipping empty space when feasible, thus optimizing inference and rendering times (What Is NeRF? - Neural Radiance Fields Explained - AWS, 2022).</p> <h3 id="advantages-over-traditional-3d-modeling">Advantages Over Traditional 3D Modeling</h3> <p>NeRF offers several advantages over conventional methods like photogrammetry or mesh modeling:</p> <p>It can naturally model complex view-dependent effects like reflections, refractions, and translucency, which are generally problematic for traditional mesh- or voxel-based approaches (Neural Radiance Fields (NeRF) Explained - Ultralytics, 2024).</p> <p>The implicit neural network-based representation is memory-efficient and can capture arbitrarily complex scenes without having to manually specify geometry or lighting (Neural Radiance Fields (NeRF) Explained - Ultralytics, 2024).</p> <p>NeRF can synthesize highly photorealistic new viewpoints and even fill in missing data between original images, a task difficult for methods reliant on dense correspondence or explicit surface reconstruction (NeRF Explained - Neural Radiance Field - Papers With Code, 2018).</p> <h3 id="limitations-of-nerf">Limitations of NeRF</h3> <p>Despite its strengths, NeRF is not without limitations:</p> <p>The computational demands for training and rendering are high—rendering a single novel viewpoint traditionally involved making multiple forward passes through the neural network for each pixel, making real-time applications challenging without further optimizations (Deep Gan Team, 2023).</p> <p>NeRF’s original formulation assumes the scene is completely static and does not handle dynamic or articulated scenes well, though more recent research is addressing these issues (arabbyuab.edu, 2024).</p> <p>The technique requires accurate camera poses for each input image, and the results are sensitive to the quantity and quality of training data (Deep Dive into NeRF (Neural Radiance Fields), 2022).</p> <h3 id="applications-of-nerf">Applications of NeRF</h3> <p>NeRF’s ability to generate photorealistic 3D visualizations from simple 2D data has spurred innovation across many domains:</p> <p>Visual effects in film and television, where NeRF can generate rich virtual sets, insert synthetic actors, or create digital doubles quickly (Neural Radiance Fields (NeRF) Explained - Ultralytics, 2024).</p> <p>Virtual and augmented reality, enabling users to explore captured environments from any viewpoint, significantly enhancing immersion (What Is NeRF? - Neural Radiance Fields Explained - AWS, 2022).</p> <p>Robotics and simulation, where realistic 3D scene modeling improves perception and navigation capabilities (What Is NeRF? - Neural Radiance Fields Explained - AWS, 2022).</p> <p>Cultural heritage preservation, digital archiving, and virtual tourism, making detailed 3D records accessible to broader audiences (The NeRF Revolution: Neural Radiance Fields Explained, 2024).</p> <h3 id="conclusion">Conclusion</h3> <p>NeRF is a groundbreaking machine learning technique that bridges 2D image datasets and fully immersive 3D environments, utilizing neural networks and advanced volume rendering to achieve levels of photorealism and flexibility previously unattainable by traditional methods (What Is NeRF? - Neural Radiance Fields Explained - AWS, 2022). While computationally intensive and best suited for static scenes in its original form, NeRF continues to evolve, shaping the future of digital content creation and 3D visualization (The NeRF Revolution: Neural Radiance Fields Explained, 2024).</p> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Jieun Ko. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>